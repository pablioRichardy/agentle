# Auto-generated from responses_api.py
# Model: ModelResponseProperties

# generated by datamodel-codegen:
#   filename:  filtered_openapi.yaml
#   timestamp: 2025-10-18T15:02:20+00:00


from typing import Optional

from pydantic import BaseModel, Field, confloat, conint


# Model dependencies
from .metadata import Metadata
from .service_tier import ServiceTier


class ModelResponseProperties(BaseModel):
    metadata: Optional[Metadata] = None
    top_logprobs: Optional[conint(ge=0, le=20)] = None
    temperature: Optional[confloat(ge=0.0, le=2.0)] = None
    top_p: Optional[confloat(ge=0.0, le=1.0)] = None
    user: Optional[str] = Field(
        None,
        description="This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use `prompt_cache_key` instead to maintain caching optimizations.\nA stable identifier for your end-users.\nUsed to boost cache hit rates by better bucketing similar requests and  to help OpenAI detect and prevent abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n",
        example="user-1234",
    )
    safety_identifier: Optional[str] = Field(
        None,
        description="A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies.\nThe IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n",
        example="safety-identifier-1234",
    )
    prompt_cache_key: Optional[str] = Field(
        None,
        description="Used by OpenAI to cache responses for similar requests to optimize your cache hit rates. Replaces the `user` field. [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n",
        example="prompt-cache-key-1234",
    )
    service_tier: Optional[ServiceTier] = None
